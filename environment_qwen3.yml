name: qwen3_omni
channels:
  - conda-forge
  - defaults
dependencies:
  - python=3.10
  - pip
  - pip:
      - accelerate>=1.10.1
      - audioread>=3.0.1
      - av>=15.1.0
      - einops>=0.8.1
      - fastapi>=0.119.0
      - huggingface-hub>=0.35.3
      - librosa>=0.11.0
      - numba>=0.61.2
      - numpy>=2.2.6
      - pyyaml>=6.0.2
      - qwen-omni-utils>=0.0.8
      - ray>=2.50.1
      - safetensors>=0.6.2
      - scikit-learn>=1.7.2
      - scipy>=1.15.3
      - sentence-transformers>=5.1.2
      - soundfile>=0.13.1
      - timm>=1.0.24
      - tokenizers>=0.22.1
      - torch>=2.7.0
      - torchaudio>=2.7.0
      - torchvision>=0.22.0
      - tqdm>=4.67.1
      - transformers>=4.57.1
      - xformers>=0.0.30
      - nnaudio>=0.3.4
      # Install vllm from specific branch for Qwen3-Omni support
      # pip install git+https://github.com/wangxiongts/vllm.git
      # Install flash-attention separately:
      # pip install flash-attn --no-build-isolation
